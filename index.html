<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sruthi Sudhakar</title>
  <!--sairam-->
  <meta name="author" content="Sruthi Sudhakar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/columbia.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sruthi Sudhakar</name>
              </p>
              <p>I am a second-year PhD student at Columbia University advised by <a href="http://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>  and <a href="https://www.cs.columbia.edu/~zemel/">Richard Zemel</a>. My work is supported by the NSF Graduate Research Fellowship. Previously, I completed my B.S. in Computer Science at Georgia Tech where I worked on Algorithmic Fairness advised by <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a> 
              </p>
              <p>
                I have had the opportunity to intern at Microsoft Research (2022) with  <a href="https://vibhav-vineet.github.io/">Vibhav Vineet</a> and <a href="https://neelj.com//">Neel Joshi</a>. I have spent 3 summers working as a Software Engineering Intern with Bloomberg and Microsoft. I am also an active mobile app development volunteer for YogaSangeeta. 
              </p>
              <p>
                In my free time, I enjoy classical indian <a href="https://www.instagram.com/ssdance_jgd9/">dancing</a> and spending time with friends and family. Feel free to email me to chat about research, grad school applications, or life! 
              </p>
              <p style="text-align:center">
                <a href="mailto:sruthisudhakar81@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Sruthi Sudhakar Resume June2024.pdf">CV</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SruthiSudhakar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sruthisudhakar_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in enabling agents to build representations of their environment and act in the world by learning from offline data and real-world interactions. Building such systems raises many facinating learning questions that I am interested in exploring including perception, generalization, compositionality, and explainability. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
    	<video controls width="160" preload="auto"  data-setup="{}" autoplay loop muted playsinline>
     		<source type="video/mp4" src="images/">
	</video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dreamitate.cs.columbia.edu/">
          <papertitle>Dreamitate: Real-World Visuomotor Policy Learning via Video Generation</papertitle>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/junbangliang/">Junban Liang</a>,
        <a href="https://www.cs.columbia.edu/~eo2464/">Ege Ozguroglu</a>,
        <a href="https://ruoshiliu.github.io/">Ruoshi Liu</a>, 
        <strong>Sruthi Sudhakar</strong>,
	<a href="https://www.achaldave.com/">Achal Dave</a>,
	<a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a>,
	<a href="https://shurans.github.io/">Shuran Song</a>,
        <a href="https://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>,
        <br>
        <em>Conference on Robot Learning 2024</em>
        <br>
        <a href="https://dreamitate.cs.columbia.edu/">arXiv</a>
        <p></p>
        <p>Dreamitate is a visuomotor policy learning framework that synthesize videos of humans using tools to complete a task, tracks the tool, and executes the trajectory on the robot to accomplish the task in the real-world.</p>
      </td>
    </tr> 
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
    	<video controls width="160" preload="auto"  data-setup="{}" autoplay loop muted playsinline>
     		<source type="video/mp4" src="images/paperbot.mp4">
	</video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://paperbot.cs.columbia.edu/">
          <papertitle>PaperBot: Learning to Design Real-World Tools Using Paper</papertitle>
        </a>
        <br>
        <a href="https://ruoshiliu.github.io/">Ruoshi Liu</a>, 
        <a href="https://www.linkedin.com/in/junbangliang/">Junban Liang</a>,
        <strong>Sruthi Sudhakar</strong>,
	<a href="https://www.cs.columbia.edu/~huy/">Huy Ha</a>,
	<a href="https://cheng-chi.github.io/">Cheng Chi</a>,
	<a href="https://shurans.github.io/">Shuran Song</a>,
        <a href="https://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>
        <br>
        <em>arxiv</em>
        <br>
        <a href="https://paperbot.cs.columbia.edu/">project page</a>, <a href="https://arxiv.org/abs/2403.09566">arXiv</a>
        <p></p>
        <p>
	PaperBot directly learns to design and use a tool in the real world using paper without human intervention.
        </p>
      </td>
    </tr> 
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
    	<video controls width="160" preload="auto"  data-setup="{}" autoplay loop muted playsinline>
     		<source type="video/mp4" src="images/teaserweb.mp4">
	</video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cv.cs.columbia.edu/sruthi/coshand/index.html">
          <papertitle>CosHAND: Controlling the World by Sleight of Hand </papertitle>
        </a>
        <br>
        <strong>Sruthi Sudhakar</strong>,
        <a href="https://ruoshiliu.github.io/">Ruoshi Liu</a>, 
        <a href="https://basile.be/about-me/">Basile Van Hoorick</a>,
        <a href="https://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>,
        <a href="https://www.cs.columbia.edu/~zemel/">Richard Zemel</a>,
        <br>
        <em>ECCV 2024 <b>(Oral, Best Paper Award Candidate)</b></em>
        <br>
        <a href="https://cv.cs.columbia.edu/sruthi/coshand/index.html">project page</a>, <a href="https://arxiv.org/abs/2408.07147">arXiv</a>
        <p></p>
        <p>
         We propose learning to model interactions through a novel form of visual conditioning: hands. Hands are a natural way to specify control through actions such as grasping, pulling, pushing, etc. Given an input image and a representation of a hand interacting with the scene, our approach, CoSHAND, synthesizes a depiction of what the scene would look like after the interaction has occurred.
        </p>
      </td>
    </tr>    
    <tr onmouseout="icon_stop()" onmouseover="icon_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='icon_image'>
            <img src='images/sim2real_after.png' width="160"></div>
          <img src='images/sim2real_before.png' width="160">
        </div>
        <script type="text/javascript">
          function icon_start() {
            document.getElementById('icon_image').style.opacity = "1";
          }

          function icon_stop() {
            document.getElementById('icon_image').style.opacity = "0";
          }
          icon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakar_Exploring_the_Sim2Real_Gap_Using_Digital_Twins_ICCV_2023_paper.pdf">
          <papertitle>Exploring the Sim2Real Gap using Digital Twins</papertitle>
        </a>
        <br>
        <strong>Sruthi Sudhakar</strong>,
        <a href="https://www.linkedin.com/in/jon-hanzelka-80701b78/">Jon Hanzelka</a>, 
        <a href="https://www.linkedin.com/in/josh-bobillot-39a0b77/">Josh Bobillot</a>,
        <a href="https://www.linkedin.com/in/tvrandhavane/">Tanmay Randhavane</a>,
        <a href="https://neelj.com/">Neel Joshi</a>,
        <a href="https://vibhav-vineet.github.io/">Vibhav Vineet</a>,
        <br>
        <em>ICCV 2023</em>
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakar_Exploring_the_Sim2Real_Gap_Using_Digital_Twins_ICCV_2023_paper.pdf">arXiv</a>
        <p></p>
        <p>
         We study factors of variation in 3D model quality between simulated and real data to determine what factors are important for computer vision model performanc. To do so, we create a new YCB-Real and digital twin YCB-Synthetic dataset to study adaptation and generalization in controlled environments.
        </p>
      </td>
    </tr>    

    <tr onmouseout="icon_stop()" onmouseover="icon_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='icon_image'>
            <img src='images/icon_after.png' width="160"></div>
          <img src='images/icon_before.png' width="160">
        </div>
        <script type="text/javascript">
          function icon_start() {
            document.getElementById('icon_image').style.opacity = "1";
          }

          function icon_stop() {
            document.getElementById('icon_image').style.opacity = "0";
          }
          icon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2306.04482">
          <papertitle>ICON2: Reliably Benchmarking Predictive Inequity in Object Detection</papertitle>
        </a>
        <br>
        <strong>Sruthi Sudhakar</strong>,
        <a href="https://virajprabhu.github.io/">Viraj Prabhu</a>, 
        <a href="https://faculty.cc.gatech.edu/~judy/">Olga Russakovsky</a>,
        <a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>,
        <br>
        <em>CVPR SSAD Workshop 2023</em>
        <br>
        <a href="https://arxiv.org/abs/2306.04482">arXiv</a>
        <p></p>
        <p>
        ICON2 leverages prior knowledge on the deficiencies of object detection systems to identify performance discrepancies across sub-populations, compute correlations between these potential confounders and a given sensitive attribute, and control for the most likely confounders to obtain a more reliable estimate of model bias.
        </p>
      </td>
    </tr>
      
    <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='alignerf_image'>
            <img src='images/bmvc_after.png' width="160"></div>
          <img src='images/bmvc_before.png' width="160">
        </div>
        <script type="text/javascript">
          function alignerf_start() {
            document.getElementById('alignerf_image').style.opacity = "1";
          }

          function alignerf_stop() {
            document.getElementById('alignerf_image').style.opacity = "0";
          }
          alignerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0282.html">
          <papertitle>Mitigating Bias in Visual Transformers via Targeted Alignment</papertitle>
        </a>
        <br>
        <strong>Sruthi Sudhakar</strong>,
        <a href="https://akrishna77.github.io/">Aravind Krishnakumar</a>,
        <a href="https://virajprabhu.github.io/">Viraj Prabhu</a>, 
				<a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>,
        <br>
        <em>BMVC 2021</em>
        <br>
        <a href="https://arxiv.org/abs/2302.04358">arXiv</a>
        <p></p>
        <p>
        TADeT: A targeted alignment strategy for debiasing transformers that aims to discover and remove bias primarily from query matrix features.
        </p>
      </td>
    </tr>
    

    <tr onmouseout="udis_stop()" onmouseover="udis_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='udis_image'>
            <img src='images/udis_after.png' width="160"></div>
          <img src='images/udis_before.png' width="160">
        </div>
        <script type="text/javascript">
          function udis_start() {
            document.getElementById('udis_image').style.opacity = "1";
          }

          function udis_stop() {
            document.getElementById('udis_image').style.opacity = "0";
          }
          udis_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0362.html">
          <papertitle>UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models</papertitle>
        </a>
        <br>
        <a href="https://akrishna77.github.io/">Aravind Krishnakumar</a>,
        <strong>Sruthi Sudhakar</strong>,
        <a href="https://virajprabhu.github.io/">Viraj Prabhu</a>, 
				<a href="https://faculty.cc.gatech.edu/~judy/">Judy Hoffman</a>,
        <br>
        <em>BMVC 2021</em>
        <br>
        <a href="https://arxiv.org/abs/2110.15499">arXiv</a>
        <p></p>
        <p>
        UDIS identifies subpopulations via hierarchical clustering of dataset embeddings and surfaces systematic failure modes by visualizing low performing clusters along with their gradient-weighted class-activation maps. 
        </p>
      </td>
    </tr>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <heading>Teaching</heading>
          <p>
            TA, Representation Learning with Prof. Carl Vondrick, Columbia University
          </p>
          <p>
            TA, Computer Vision with Prof. Judy Hoffman , Georgia Tech
          </p>
        </td>
      </tr>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website Credit, <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
